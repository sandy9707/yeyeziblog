<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>魔搭 ModelScope 介绍和使用实录 (一) | Yezi's Hexo Blog</title><meta name="author" content="Yezi"><meta name="copyright" content="Yezi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="引言其实，我对魔搭 ModelScope 是有一些感情在的，因为我的第一次大模型微调经验就是基于魔搭平台，并且凭此获得了数字中国创新大赛的奖项。 有趣的是，出于种种机缘巧合，我既缺乏基础知识，也没有接受过完整的培训，甚至没有经历全流程的使用。 由于当时急于求成，虽然成功完成了训练，却没能深入探索魔搭的强大功能，仅使用了 swift 便捷的训练功能。 所以，这次我打算从”Quick Start”开始">
<meta property="og:type" content="article">
<meta property="og:title" content="魔搭 ModelScope 介绍和使用实录 (一)">
<meta property="og:url" content="https://yeyeziblog.eu.org/2024/11/05/Tech/LLM/modelscope1/index.html">
<meta property="og:site_name" content="Yezi&#39;s Hexo Blog">
<meta property="og:description" content="引言其实，我对魔搭 ModelScope 是有一些感情在的，因为我的第一次大模型微调经验就是基于魔搭平台，并且凭此获得了数字中国创新大赛的奖项。 有趣的是，出于种种机缘巧合，我既缺乏基础知识，也没有接受过完整的培训，甚至没有经历全流程的使用。 由于当时急于求成，虽然成功完成了训练，却没能深入探索魔搭的强大功能，仅使用了 swift 便捷的训练功能。 所以，这次我打算从”Quick Start”开始">
<meta property="og:locale">
<meta property="og:image" content="https://yeyeziblog.eu.org/img/modelscope1_2024-11-06-11-08-32.png">
<meta property="article:published_time" content="2024-11-05T12:27:30.000Z">
<meta property="article:modified_time" content="2024-11-05T12:27:30.000Z">
<meta property="article:author" content="Yezi">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yeyeziblog.eu.org/img/modelscope1_2024-11-06-11-08-32.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yeyeziblog.eu.org/2024/11/05/Tech/LLM/modelscope1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2155310374265743" crossorigin="anonymous"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"74HMPRFIN5","apiKey":"896979d16b9d8fcb17ef98c222d6d40a","indexName":"yeziblog","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '魔搭 ModelScope 介绍和使用实录 (一)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-05 20:27:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">94</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">42</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Yezi's Hexo Blog"><span class="site-name">Yezi's Hexo Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">魔搭 ModelScope 介绍和使用实录 (一)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-11-05T12:27:30.000Z" title="Created 2024-11-05 20:27:30">2024-11-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-11-05T12:27:30.000Z" title="Updated 2024-11-05 20:27:30">2024-11-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Tech/">Tech</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Tech/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="魔搭 ModelScope 介绍和使用实录 (一)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>其实，我对魔搭 ModelScope 是有一些感情在的，因为我的第一次大模型微调经验就是基于魔搭平台，并且凭此获得了数字中国创新大赛的奖项。</p>
<p>有趣的是，出于种种机缘巧合，我既缺乏基础知识，也没有接受过完整的培训，甚至没有经历全流程的使用。</p>
<p>由于当时急于求成，虽然成功完成了训练，却没能深入探索魔搭的强大功能，仅使用了 swift 便捷的训练功能。</p>
<p>所以，这次我打算从”Quick Start”开始，认真学习魔搭的每一个功能。</p>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><h3 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h3><p>很奇妙的一点，中英文的教程竟然是不同的，英文版在概念解释上更详细，而中文版在操作流程上更详细。然后，中文版似乎更新更频繁，所以如果是国内用户并以使用优先，建议优先参考中文版，但如果想要更深入的了解模型训练的原理，建议参考英文版。</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.modelscope.cn/docs/%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B">魔搭社区</a></li>
<li><a target="_blank" rel="noopener" href="https://www.modelscope.cn/docs/Quick%20Start">魔搭社区</a></li>
</ol>
<p>哦，对了，如果是打算用的话，建议去直接看文档，他写的真的很好，真的可以让你在 5 分钟内学会使用。</p>
<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><p>首先是一些基础概念：</p>
<p>Task（任务）:<br>特定领域的具体应用，如图像分类、文本生成等，用户可根据输入输出需求选择合适的模型。</p>
<blockquote>
<p>Task refers to a specific application in a certain field, used to complete tasks in specific scenarios. For example, image classification, text generation, speech recognition, etc. You can find the task type suitable for your application scenario based on the input and output of the task, and filter the tasks to find the model you need.</p>
</blockquote>
<p>Model（模型）:<br>包含网络结构和参数的具体模型实例，提供实际的 AI 能力。</p>
<blockquote>
<p>Model refers to a specific model instance, including the model network structure and corresponding parameters. ModelScope provides rich model information for users to experience and use.</p>
</blockquote>
<p>Modelhub（模型库）:<br>模型库是模型资源的管理和分发平台，提供丰富的模型资源，帮助用户快速找到合适的模型。</p>
<blockquote>
<p>Model library (Modelhub) refers to model services that store, version manage and related operations for models. Models uploaded and shared by users will be stored in ModelScope’s model library. Users can also create their own model storage in Model hub. library, and continue to use the model library management function provided by the platform for model management.</p>
</blockquote>
<p>Dataset（数据集）:<br>用于算法训练、测试的数据集合，支持模型训练和验证。</p>
<blockquote>
<p>Dataset refers to a set of data used for model training and evaluation. ModelScope provides rich dataset information for users to experience and use.</p>
</blockquote>
<p>Datasethub（数据集库）:<br>集中管理数据的平台，便于数据访问、管理和共享。</p>
<blockquote>
<p>Datasethub is used to centrally manage data and support model training, prediction, etc., making various types of data easy to access, manage, and share.</p>
</blockquote>
<p>ModelScope Library（模型库）:<br>ModelScope 的 Python 框架库，提供简单的代码接口实现模型推理、训练等功能。</p>
<blockquote>
<p>ModelScope Library is a set of Python Library frameworks developed by ModelScope. By calling specific methods, users can write just a few lines of code to complete tasks such as model reasoning, training, and evaluation. On this basis, they can also quickly Carry out secondary development and realize your own innovative ideas.</p>
</blockquote>
<h3 id="Model-Exploration"><a href="#Model-Exploration" class="headerlink" title="Model Exploration"></a>Model Exploration</h3><p>模型探索：</p>
<p>访问平台网站 <a target="_blank" rel="noopener" href="https://www.modelscope.cn/models">https://www.modelscope.cn/models</a>，可以看到平台上的所有公共模型，通过任务或关键词搜索找到感兴趣的模型。</p>
<blockquote>
<p>如果需要找到可以在线体验或支持训练和微调的模型，可以通过搜索框右侧的过滤框进行过滤。可以在线体验和训练的模型将不断丰富和扩展，敬请期待。<br>本文以中文分词模型为例，带你体验使用 ModelScope 的模型。<br>可以通过过滤任务为 “词性标注” 或搜索关键词 “词性标注” 来检索对应的词性标注模型。在模型卡片右侧，会显示模型的基本信息，包括中文和英文名称、任务类型标签和其他模型标签、模型使用的深度学习框架、模型提供者、下载次数、点赞次数、模型描述等信息，帮助你快速理解模型。</p>
</blockquote>
<p><img src="https://resouces.modelscope.cn/document/docdata/2024-11-4_11:14/dist/Beginner's%20Guide/resources/model_exploartion.png" alt="public models on the platform"></p>
<blockquote>
<p>点击模型进入模型详情页，可以查看模型的具体介绍。模型提供者会在这里介绍模型的具体应用场景、功能描述、技术方法、模型训练和使用、效果评估等信息，供你参考。<br>在模型卡片右侧，有一个 “在线体验” 功能。可以切换默认示例查看模型效果，或输入适合模型的测试信息测试模型效果。</p>
</blockquote>
<p><img src="https://resouces.modelscope.cn/document/docdata/2024-11-4_11:14/dist/Beginner's%20Guide/resources/word_segmentation_online.png" alt="model details page"></p>
<p>事实上并非所有模型都支持在线体验，但是依然是非常实用的功能。</p>
<blockquote>
<p>在开始在线体验之前，请仔细阅读并理解相关的免责声明，合法合规地使用在线体验模块的功能。<br>除了模型体验，你还可以查看和下载模型的具体文件。（目前支持通过 git 和 Python SDK 下载。更多信息请参见模型下载）<br>如果你对模型效果满意，想要了解更多并进行代码验证，可以点击 “快速使用” 查看具体的代码示例方法。平台提供了两种下载模型的方式：</p>
<ol>
<li>Install and download through ModelScope Library</li>
<li>Pull the repository through git<br>这些方法都可以在本地运行模型。但是，你需要先安装 ModelScope Library 才能运行代码。详情请参见步骤 [环境准备] - [本地开发环境安装]。</li>
</ol>
</blockquote>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="本地开发环境安装"><a href="#本地开发环境安装" class="headerlink" title="本地开发环境安装"></a>本地开发环境安装</h4><p>如果你需要运行模型，你需要进行相应的环境安装准备，包括：</p>
<ul>
<li>安装 python 环境。支持 python3，但不支持 python2。推荐使用 Anaconda 安装。</li>
<li>安装深度学习框架。ModelScope Library 目前支持两大深度学习框架，Tensorflow 和 Pytorch，用于模型训练和推理。你可以根据模型所需的框架选择合适的框架进行安装。</li>
<li>安装 ModelScope Library。我们提供了两种安装方法，你可以根据需要选择合适的安装方法。<ul>
<li>pip 安装。ModelScope 根据不同领域提供了安装包，你可以根据对应的模型选择所需的安装包。</li>
<li>从源码安装。</li>
</ul>
</li>
</ul>
<p>魔搭社区提供了预安装好的 Notebook 环境来使用魔搭社区模型，并为新用户提供了 100 小时的免费 GPU 算力和不限时长的免费 CPU 算力，并预安装了大部分模型可运行的环境依赖。</p>
<h4 id="Python-环境安装配置"><a href="#Python-环境安装配置" class="headerlink" title="Python 环境安装配置"></a>Python 环境安装配置</h4><p>首先，参考文档 安装配置 Anaconda 环境。安装完成后，执行如下命令为 ModelScope library 创建对应的 Python 环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n modelscope python=3.10</span><br><span class="line">conda activate modelscope</span><br></pre></td></tr></table></figure>

<h4 id="ModelScope-Library-安装"><a href="#ModelScope-Library-安装" class="headerlink" title="ModelScope Library 安装"></a>ModelScope Library 安装</h4><p>ModelScope Library 由核心 hub 支持，框架，以及不同领域模型的对接组件组成。根据您实际使用的场景，可以选择不同的安装选项。</p>
<p>如果只需要通过 ModelScope SDK，或者 ModelScope 命令行工具来下载模型，可以只最轻量化的安装 ModelScope 的核心 hub 支持：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope</span><br></pre></td></tr></table></figure>

<p>如果需要更完整的使用 ModelScope 平台上的一系列框架能力，包括数据集的加载，外部模型的使用等，则推荐使用”framework”的安装选项，也就是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install modelscope[framework]</span><br></pre></td></tr></table></figure>

<p>以上两种方法，都不涉及 ModelScope 上原生模型的集成。要使用 ModelScope 来实现各种领域模型的使用，包括基于 NLP、CV、语音、多模态，等不同领域的模型，来进行模型推理以及模型训练、微调等能力，则需要根据具体领域，通过安装选项，来安装额外的依赖。同时也涉及对应的 PyTorch,Tensorflow 等机器学习框架的安装。</p>
<p>同时社区推荐安装 Git 和 Git LFS，这是模型管理包括上传所必须的工具。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>以下部分的内容建议参考文档，因为文档写的真的很好。</p>
<ul>
<li>深度学习框架依赖的安装</li>
<li>分领域 ModelScope 模型依赖的安装</li>
<li>安装验证</li>
</ul>
<h4 id="实际流程"><a href="#实际流程" class="headerlink" title="实际流程"></a>实际流程</h4><p>因为我这次是为了体验 cv 领域的模型，所以选择了安装<code>modelscope[cv]</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install torch</span><br><span class="line">pip install <span class="string">&quot;modelscope[cv]&quot;</span> -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html</span><br></pre></td></tr></table></figure>

<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>如果您在高带宽的机器上运行，推荐使用 ModelScope 命令行工具下载模型。该方法支持断点续传和模型高速下载，例如可以通过如下命令，将 Qwen2.5-0.5B-Instruct 模型，下载到当前路径下的”model-dir”目录。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model Qwen/Qwen2.5-0.5B-Instruct --local_dir ./model-dir</span><br></pre></td></tr></table></figure>

<p>您也可以使用 ModelScope Python SDK 下载模型，该方法支持断点续传和模型高速下载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line">model_dir = snapshot_download(<span class="string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>由于模型都是通过 Git 存储，所以也可以在安装 Git LFS 后，通过 git clone 的方式在本地下载模型，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git <span class="built_in">clone</span> https://www.modelscope.cn/Qwen/Qwen2.5-0.5B-Instruct.git</span><br></pre></td></tr></table></figure>

<p>关于模型下载的详细说明，可参考模型下载文档。</p>
<p>同时，如果模型和 ModelScope SDK 绑定，则只需要几行代码即可加载模型，同时 ModelScope 还支持通过 AutoModel 等接口来加载模型。如下是使用 AutoModel 和 pipeline 方式加载模型的示例：</p>
<h4 id="实际操作"><a href="#实际操作" class="headerlink" title="实际操作"></a>实际操作</h4><p>我实验的模型是图片生成模型：<code>PAI中文Diffusion模型-美食</code>。</p>
<p><a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/PAI/pai-diffusion-food-large-zh">魔搭社区</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 命令行下载</span></span><br><span class="line">modelscope download --model PAI/pai-diffusion-food-large-zh --local_dir ./model</span><br></pre></td></tr></table></figure>

<h3 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h3><h4 id="使用-ModelScope-pipeline-加载模型"><a href="#使用-ModelScope-pipeline-加载模型" class="headerlink" title="使用 ModelScope pipeline 加载模型"></a>使用 ModelScope pipeline 加载模型</h4><p>如果你准备好了环境，可以基于以下代码推理模型。使用 modelscope pipeline 接口只需要两步，以下以词性标注模型（damo&#x2F;nlp_structbert_word-segmentation_chinese-base）为例进行简要说明：</p>
<p>首先实例化一个基于任务的 pipeline 对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line">word_segmentation = pipeline(<span class="string">&#x27;word-segmentation&#x27;</span>,model=<span class="string">&#x27;damo/nlp_structbert_word-segmentation_chinese-base&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>输入数据并获取输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_str = <span class="string">&#x27;今天天气不错，适合出去游玩&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(word_segmentation(input_str))</span><br><span class="line">&#123;<span class="string">&#x27;output&#x27;</span>: <span class="string">&#x27;今天 天气 不错，适合 出去 游玩&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<h4 id="使用-AutoModel-加载模型"><a href="#使用-AutoModel-加载模型" class="headerlink" title="使用 AutoModel 加载模型"></a>使用 AutoModel 加载模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br></pre></td></tr></table></figure>

<h3 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h3><h4 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h4><p>推理不同模态多种任务，pipeline 是最简单、最快捷的方法。您可以使用开箱即用的 pipeline 执行跨不同模式的多种任务，下面是一个 pipeline 完整的运行示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">from</span> modelscope.utils.constant <span class="keyword">import</span> Tasks</span><br><span class="line"></span><br><span class="line">inference_pipeline = pipeline(</span><br><span class="line">    task=Tasks.auto_speech_recognition,</span><br><span class="line">    model=<span class="string">&#x27;iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch&#x27;</span>,</span><br><span class="line">    model_revision=<span class="string">&quot;v2.0.4&quot;</span>)</span><br><span class="line"></span><br><span class="line">rec_result = inference_pipeline(<span class="string">&#x27;https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_vad_punc_example.wav&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(rec_result)</span><br></pre></td></tr></table></figure>

<h4 id="AutoModel-和-AutoTokenizer"><a href="#AutoModel-和-AutoTokenizer" class="headerlink" title="AutoModel 和 AutoTokenizer"></a>AutoModel 和 AutoTokenizer</h4><p>ModelScope 兼容了 Transformers 提供的简单而统一的方法来加载预训练实例和 tokenizer。这意味着您可以使用 ModelScope 加载 AutoModel 和 AutoTokenizer 等类。下面是一个大语言模型的完整的运行示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;qwen/Qwen2.5-0.5B-Instruct&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    torch_dtype=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;Give me a short introduction to large language model.&quot;</span></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize=<span class="literal">False</span>,</span><br><span class="line">    add_generation_prompt=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">model_inputs = tokenizer([text], return_tensors=<span class="string">&quot;pt&quot;</span>).to(model.device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    **model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line">generated_ids = [</span><br><span class="line">    output_ids[<span class="built_in">len</span>(input_ids):] <span class="keyword">for</span> input_ids, output_ids <span class="keyword">in</span> <span class="built_in">zip</span>(model_inputs.input_ids, generated_ids)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode[generated_ids, skip_special_tokens=<span class="literal">True</span>](<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h4 id="其他模型推理"><a href="#其他模型推理" class="headerlink" title="其他模型推理"></a>其他模型推理</h4><p>对于暂时未与 ModelScope SDK 做原生集成的模型，可以先从 ModelScope 上下载模型，然后通过其他的主流库实现模型推理，以 SDXL-Turbo 模型为例，完整的模型推理运行示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> AutoPipelineForText2Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> modelscope <span class="keyword">import</span> snapshot_download</span><br><span class="line"></span><br><span class="line">model_dir = snapshot_download(<span class="string">&quot;AI-ModelScope/sdxl-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line">pipe = AutoPipelineForText2Image.from_pretrained(model_dir, torch_dtype=torch.float16, variant=<span class="string">&quot;fp16&quot;</span>)</span><br><span class="line">pipe.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;A cinematic shot of a baby racoon wearing an intricate italian priest robe.&quot;</span></span><br><span class="line"></span><br><span class="line">image = pipe(prompt=prompt, num_inference_steps=<span class="number">1</span>, guidance_scale=<span class="number">0.0</span>).images[<span class="number">0</span>]</span><br><span class="line">image.save(<span class="string">&quot;image.png&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="模型推理实际操作"><a href="#模型推理实际操作" class="headerlink" title="模型推理实际操作"></a>模型推理实际操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> modelscope.pipelines <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">p = pipeline(<span class="string">&#x27;text-to-image-synthesis&#x27;</span>, <span class="string">&#x27;PAI/pai-diffusion-food-large-zh&#x27;</span>, model_revision=<span class="string">&#x27;v1.0.0&#x27;</span>)</span><br><span class="line">result = p(&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;红烧狮子头&#x27;</span>&#125;)</span><br><span class="line">image = result[<span class="string">&quot;output_imgs&quot;</span>][<span class="number">0</span>]</span><br><span class="line">cv2.imwrite(<span class="string">&quot;image.png&quot;</span>, image)</span><br></pre></td></tr></table></figure>

<p>注：可以把模型地址写为刚刚下载的文件夹路径。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>受益匪浅，ModelScope 的文档写的真的很好，而且很详细，很适合我这种小白。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://yeyeziblog.eu.org">Yezi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yeyeziblog.eu.org/2024/11/05/Tech/LLM/modelscope1/">https://yeyeziblog.eu.org/2024/11/05/Tech/LLM/modelscope1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="/img/modelscope1_2024-11-06-11-08-32.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/11/06/Tech/LLM/modelscope2/" title="魔搭 ModelScope (二)"><img class="cover" src="/img/modelscope1_2024-11-06-11-08-32.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">魔搭 ModelScope (二)</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/04/Tech/LLM/%E9%A3%9F%E7%89%A9%E8%AF%86%E5%88%AB%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83/" title="食物识别大模型能力比较"><img class="cover" src="/img/%E5%8F%B6%E5%AD%90%E7%9A%84%E7%A5%9E%E5%A5%87%E5%B0%8F%E8%BD%AF%E4%BB%B6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">食物识别大模型能力比较</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yezi</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">94</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">42</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/sandy9707"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">2.</span> <span class="toc-text">效果展示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Quick-Start"><span class="toc-number">3.1.</span> <span class="toc-text">Quick Start</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">3.2.</span> <span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Model-Exploration"><span class="toc-number">3.3.</span> <span class="toc-text">Model Exploration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="toc-number">3.4.</span> <span class="toc-text">环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="toc-number">3.4.1.</span> <span class="toc-text">本地开发环境安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Python-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE"><span class="toc-number">3.4.2.</span> <span class="toc-text">Python 环境安装配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ModelScope-Library-%E5%AE%89%E8%A3%85"><span class="toc-number">3.4.3.</span> <span class="toc-text">ModelScope Library 安装</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">3.4.4.</span> <span class="toc-text">其他</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%B5%81%E7%A8%8B"><span class="toc-number">3.4.5.</span> <span class="toc-text">实际流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"><span class="toc-number">3.5.</span> <span class="toc-text">模型下载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C"><span class="toc-number">3.5.1.</span> <span class="toc-text">实际操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD"><span class="toc-number">3.6.</span> <span class="toc-text">模型加载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-ModelScope-pipeline-%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.6.1.</span> <span class="toc-text">使用 ModelScope pipeline 加载模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-AutoModel-%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.6.2.</span> <span class="toc-text">使用 AutoModel 加载模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="toc-number">3.7.</span> <span class="toc-text">模型推理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pipeline"><span class="toc-number">3.7.1.</span> <span class="toc-text">pipeline</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AutoModel-%E5%92%8C-AutoTokenizer"><span class="toc-number">3.7.2.</span> <span class="toc-text">AutoModel 和 AutoTokenizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="toc-number">3.7.3.</span> <span class="toc-text">其他模型推理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C"><span class="toc-number">3.7.4.</span> <span class="toc-text">模型推理实际操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">4.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/11/06/Tech/LLM/modelscope2/" title="魔搭 ModelScope (二)"><img src="/img/modelscope1_2024-11-06-11-08-32.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="魔搭 ModelScope (二)"/></a><div class="content"><a class="title" href="/2024/11/06/Tech/LLM/modelscope2/" title="魔搭 ModelScope (二)">魔搭 ModelScope (二)</a><time datetime="2024-11-06T03:05:05.523Z" title="Created 2024-11-06 11:05:05">2024-11-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/05/Tech/LLM/modelscope1/" title="魔搭 ModelScope 介绍和使用实录 (一)"><img src="/img/modelscope1_2024-11-06-11-08-32.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="魔搭 ModelScope 介绍和使用实录 (一)"/></a><div class="content"><a class="title" href="/2024/11/05/Tech/LLM/modelscope1/" title="魔搭 ModelScope 介绍和使用实录 (一)">魔搭 ModelScope 介绍和使用实录 (一)</a><time datetime="2024-11-05T12:27:30.000Z" title="Created 2024-11-05 20:27:30">2024-11-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/04/Tech/LLM/%E9%A3%9F%E7%89%A9%E8%AF%86%E5%88%AB%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83/" title="食物识别大模型能力比较"><img src="/img/%E5%8F%B6%E5%AD%90%E7%9A%84%E7%A5%9E%E5%A5%87%E5%B0%8F%E8%BD%AF%E4%BB%B6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="食物识别大模型能力比较"/></a><div class="content"><a class="title" href="/2024/11/04/Tech/LLM/%E9%A3%9F%E7%89%A9%E8%AF%86%E5%88%AB%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83/" title="食物识别大模型能力比较">食物识别大模型能力比较</a><time datetime="2024-11-04T03:29:28.000Z" title="Created 2024-11-04 11:29:28">2024-11-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/02/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/R/rserver%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8conda/" title="rserver 中无法使用 conda 的解决办法"><img src="/img/%E5%8F%B6%E5%AD%90%E7%9A%84%E7%A5%9E%E5%A5%87%E5%B0%8F%E8%BD%AF%E4%BB%B6.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="rserver 中无法使用 conda 的解决办法"/></a><div class="content"><a class="title" href="/2024/11/02/%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6/R/rserver%E4%B8%AD%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8conda/" title="rserver 中无法使用 conda 的解决办法">rserver 中无法使用 conda 的解决办法</a><time datetime="2024-11-02T12:28:59.000Z" title="Created 2024-11-02 20:28:59">2024-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/02/%E6%9D%82%E8%B0%88/%E5%90%8C%E4%BA%BA%E6%96%87/%E8%B5%B0%E8%BF%9B%E4%BF%AE%E4%BB%992024%E5%90%8C%E4%BA%BA%E6%B4%BB%E5%8A%A8/" title="走进修仙 2024 同人活动"><img src="/img/%E8%B5%B0%E8%BF%9B%E4%BF%AE%E4%BB%99.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="走进修仙 2024 同人活动"/></a><div class="content"><a class="title" href="/2024/11/02/%E6%9D%82%E8%B0%88/%E5%90%8C%E4%BA%BA%E6%96%87/%E8%B5%B0%E8%BF%9B%E4%BF%AE%E4%BB%992024%E5%90%8C%E4%BA%BA%E6%B4%BB%E5%8A%A8/" title="走进修仙 2024 同人活动">走进修仙 2024 同人活动</a><time datetime="2024-11-02T04:12:00.000Z" title="Created 2024-11-02 12:12:00">2024-11-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Yezi</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: 'a1ca824ebb41b4570f94',
      clientSecret: '48b6c61bc875bb16f99774f4a271005632d6400a',
      repo: 'Yeziblog-Comments',
      owner: 'sandy9707',
      admin: ['sandy9707'],
      id: '5f0b9fa0b455811e8a1010caf2fd9fce',
      updateCountCallback: commentCount
    },))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>